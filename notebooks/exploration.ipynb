{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Demand Reconciliation with Temporal Anomaly Feedback\n",
    "\n",
    "This notebook demonstrates the hierarchical forecasting system with temporal anomaly feedback on synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path().absolute().parent / \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_demand_reconciliation_with_temporal_anomaly_feedback.data.loader import (\n",
    "    M5DataLoader,\n",
    "    HierarchicalTimeSeriesDataset,\n",
    ")\n",
    "from hierarchical_demand_reconciliation_with_temporal_anomaly_feedback.data.preprocessing import (\n",
    "    HierarchicalPreprocessor,\n",
    ")\n",
    "from hierarchical_demand_reconciliation_with_temporal_anomaly_feedback.models.model import (\n",
    "    HierarchicalReconciliationTransformer,\n",
    ")\n",
    "from hierarchical_demand_reconciliation_with_temporal_anomaly_feedback.evaluation.metrics import (\n",
    "    HierarchicalMetrics,\n",
    ")\n",
    "from hierarchical_demand_reconciliation_with_temporal_anomaly_feedback.utils.config import (\n",
    "    load_config,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Hierarchical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_hierarchical_data(n_items=30, n_days=180, seed=42):\n",
    "    \"\"\"Create synthetic hierarchical time series data.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Hierarchy structure\n",
    "    n_stores = 5\n",
    "    n_depts = 3\n",
    "    n_cats = 2\n",
    "    n_states = 2\n",
    "    \n",
    "    # Create items\n",
    "    items = [f\"ITEM_{i:03d}\" for i in range(n_items)]\n",
    "    stores = [f\"{['CA', 'TX'][i%n_states]}_{(i%n_stores):01d}\" for i in range(n_items)]\n",
    "    depts = [f\"DEPT_{i//10:01d}\" for i in range(n_items)]\n",
    "    cats = [f\"CAT_{i//15:01d}\" for i in range(n_items)]\n",
    "    states = [store.split('_')[0] for store in stores]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i, (item_id, store_id, dept_id, cat_id, state_id) in enumerate(\n",
    "        zip(items, stores, depts, cats, states)\n",
    "    ):\n",
    "        # Base demand pattern\n",
    "        base_level = 10 + np.random.uniform(-3, 3)\n",
    "        trend = np.random.uniform(-0.01, 0.01)\n",
    "        \n",
    "        # Weekly seasonality\n",
    "        weekly_pattern = 3 * np.sin(2 * np.pi * np.arange(n_days) / 7 + np.random.uniform(0, 2*np.pi))\n",
    "        \n",
    "        # Random anomalies\n",
    "        anomaly_days = np.random.choice(n_days, size=np.random.randint(5, 15), replace=False)\n",
    "        anomaly_effect = np.zeros(n_days)\n",
    "        anomaly_effect[anomaly_days] = np.random.uniform(5, 20, len(anomaly_days))\n",
    "        \n",
    "        # Combine effects\n",
    "        sales_pattern = (\n",
    "            base_level\n",
    "            + trend * np.arange(n_days)\n",
    "            + weekly_pattern\n",
    "            + anomaly_effect\n",
    "            + np.random.normal(0, 1, n_days)\n",
    "        )\n",
    "        \n",
    "        # Ensure non-negative\n",
    "        sales_pattern = np.maximum(0.1, sales_pattern)\n",
    "        \n",
    "        for day in range(n_days):\n",
    "            data.append({\n",
    "                \"item_id\": item_id,\n",
    "                \"dept_id\": dept_id,\n",
    "                \"cat_id\": cat_id,\n",
    "                \"store_id\": store_id,\n",
    "                \"state_id\": state_id,\n",
    "                \"d\": f\"d_{day + 1}\",\n",
    "                \"date\": pd.Timestamp(\"2021-01-01\") + pd.Timedelta(days=day),\n",
    "                \"sales\": sales_pattern[day],\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating synthetic hierarchical data...\")\n",
    "sales_data = create_synthetic_hierarchical_data()\n",
    "print(f\"Generated {len(sales_data)} records for {sales_data['item_id'].nunique()} items\")\n",
    "print(f\"Date range: {sales_data['date'].min()} to {sales_data['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview\n",
    "print(\"Data Overview:\")\n",
    "print(f\"Shape: {sales_data.shape}\")\n",
    "print(f\"Columns: {sales_data.columns.tolist()}\")\n",
    "print(f\"Unique items: {sales_data['item_id'].nunique()}\")\n",
    "print(f\"Unique stores: {sales_data['store_id'].nunique()}\")\n",
    "print(f\"Unique departments: {sales_data['dept_id'].nunique()}\")\n",
    "print(f\"Date range: {(sales_data['date'].max() - sales_data['date'].min()).days} days\")\n",
    "\n",
    "# Sales statistics\n",
    "print(\"\\nSales Statistics:\")\n",
    "print(sales_data['sales'].describe())\n",
    "\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample time series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Sample 4 items\n",
    "sample_items = sales_data['item_id'].unique()[:4]\n",
    "\n",
    "for i, item in enumerate(sample_items):\n",
    "    item_data = sales_data[sales_data['item_id'] == item].sort_values('date')\n",
    "    axes[i].plot(item_data['date'], item_data['sales'], linewidth=1)\n",
    "    axes[i].set_title(f'Sales for {item}')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Sales')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchy analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sales distribution by hierarchy level\n",
    "store_sales = sales_data.groupby(['store_id', 'date'])['sales'].sum().reset_index()\n",
    "dept_sales = sales_data.groupby(['dept_id', 'date'])['sales'].sum().reset_index()\n",
    "total_sales = sales_data.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Item level\n",
    "item_daily_avg = sales_data.groupby('item_id')['sales'].mean()\n",
    "axes[0].hist(item_daily_avg, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Average Daily Sales Distribution\\n(Item Level)')\n",
    "axes[0].set_xlabel('Average Daily Sales')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Store level\n",
    "store_daily_avg = store_sales.groupby('store_id')['sales'].mean()\n",
    "axes[1].hist(store_daily_avg, bins=10, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1].set_title('Average Daily Sales Distribution\\n(Store Level)')\n",
    "axes[1].set_xlabel('Average Daily Sales')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Total level time series\n",
    "axes[2].plot(total_sales['date'], total_sales['sales'], linewidth=2, color='green')\n",
    "axes[2].set_title('Total Sales Over Time')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Total Sales')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = HierarchicalPreprocessor(\n",
    "    sequence_length=14,  # 2 weeks input\n",
    "    prediction_length=7, # 1 week prediction\n",
    "    anomaly_window=7,\n",
    "    anomaly_threshold=0.1,\n",
    ")\n",
    "\n",
    "# Split data\n",
    "train_data, val_data, test_data = preprocessor.split_data(\n",
    "    sales_data, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Data splits:\")\n",
    "print(f\"  Train: {len(train_data)} records\")\n",
    "print(f\"  Val: {len(val_data)} records\")\n",
    "print(f\"  Test: {len(test_data)} records\")\n",
    "\n",
    "# Fit preprocessor on training data\n",
    "preprocessor.fit(train_data)\n",
    "\n",
    "# Transform data\n",
    "train_processed = preprocessor.transform(train_data)\n",
    "val_processed = preprocessor.transform(val_data)\n",
    "\n",
    "print(f\"\\nProcessed data columns: {train_processed.columns.tolist()}\")\n",
    "print(f\"Anomalies detected in training: {train_processed['is_anomaly'].sum()} / {len(train_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly detection\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Sample item for visualization\n",
    "sample_item = train_processed['item_id'].iloc[0]\n",
    "item_data = train_processed[train_processed['item_id'] == sample_item].sort_values('date')\n",
    "\n",
    "# Original sales with anomalies highlighted\n",
    "axes[0].plot(item_data['date'], item_data['sales'], 'b-', linewidth=1, label='Sales')\n",
    "anomaly_points = item_data[item_data['is_anomaly'] == True]\n",
    "if len(anomaly_points) > 0:\n",
    "    axes[0].scatter(anomaly_points['date'], anomaly_points['sales'], \n",
    "                   color='red', s=50, label='Detected Anomalies', zorder=5)\n",
    "axes[0].set_title(f'Sales Time Series with Anomaly Detection: {sample_item}')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Sales')\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Anomaly scores\n",
    "axes[1].plot(item_data['date'], item_data['anomaly_score'], 'g-', linewidth=1, label='Anomaly Score')\n",
    "axes[1].axhline(y=0.5, color='red', linestyle='--', label='Threshold')\n",
    "axes[1].set_title('Anomaly Scores Over Time')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Anomaly Score')\n",
    "axes[1].legend()\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "hierarchy_mapping = {\n",
    "    \"item_id\": 0,\n",
    "    \"dept_id\": 1,\n",
    "    \"cat_id\": 2,\n",
    "    \"store_id\": 3,\n",
    "    \"state_id\": 4,\n",
    "    \"total\": 5,\n",
    "}\n",
    "\n",
    "train_dataset = HierarchicalTimeSeriesDataset(\n",
    "    data=train_processed,\n",
    "    sequence_length=14,\n",
    "    prediction_length=7,\n",
    "    hierarchy_mapping=hierarchy_mapping,\n",
    "    features=[\"sales\"],\n",
    ")\n",
    "\n",
    "val_dataset = HierarchicalTimeSeriesDataset(\n",
    "    data=val_processed,\n",
    "    sequence_length=14,\n",
    "    prediction_length=7,\n",
    "    hierarchy_mapping=hierarchy_mapping,\n",
    "    features=[\"sales\"],\n",
    ")\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "\n",
    "# Examine a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample structure:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple reconciliation matrix for demonstration\n",
    "n_levels = len(hierarchy_mapping)\n",
    "reconciliation_matrix = torch.eye(n_levels, dtype=torch.float32)\n",
    "\n",
    "# Initialize model\n",
    "model = HierarchicalReconciliationTransformer(\n",
    "    input_dim=1,\n",
    "    hidden_size=64,  # Smaller for demo\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    "    prediction_length=7,\n",
    "    num_hierarchy_levels=n_levels,\n",
    "    reconciliation_matrix=reconciliation_matrix,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model initialized:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model forward pass\n",
    "model.eval()\n",
    "\n",
    "# Create a small batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(batch[\"input\"], batch[\"hierarchy_level\"])\n",
    "\n",
    "print(f\"\\nModel outputs:\")\n",
    "for key, value in outputs.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape {value.shape}, range [{value.min():.3f}, {value.max():.3f}]\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training (Mini Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training demo (just a few steps for illustration)\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop (just a few batches for demo)\n",
    "train_losses = []\n",
    "n_demo_batches = 10\n",
    "\n",
    "print(\"Running mini training demo...\")\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if batch_idx >= n_demo_batches:\n",
    "        break\n",
    "        \n",
    "    # Forward pass\n",
    "    outputs = model(batch[\"input\"], batch[\"hierarchy_level\"])\n",
    "    \n",
    "    # Compute loss (simplified)\n",
    "    forecast_loss = F.mse_loss(outputs[\"base_forecasts\"], batch[\"target\"])\n",
    "    reconciliation_loss = F.mse_loss(outputs[\"reconciled_forecasts\"], batch[\"target\"])\n",
    "    coherence_loss = outputs[\"coherence_loss\"]\n",
    "    \n",
    "    total_loss = forecast_loss + 0.3 * reconciliation_loss + 0.1 * coherence_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(total_loss.item())\n",
    "    \n",
    "    if batch_idx % 3 == 0:\n",
    "        print(f\"  Batch {batch_idx}: Loss = {total_loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nDemo training completed. Final loss: {train_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, 'b-', linewidth=2, marker='o')\n",
    "plt.title('Training Loss During Mini Demo')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_base_predictions = []\n",
    "all_reconciled_predictions = []\n",
    "all_anomaly_scores = []\n",
    "all_hierarchy_levels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        outputs = model(batch[\"input\"], batch[\"hierarchy_level\"])\n",
    "        \n",
    "        all_predictions.append(outputs[\"reconciled_forecasts\"].numpy())\n",
    "        all_targets.append(batch[\"target\"].numpy())\n",
    "        all_base_predictions.append(outputs[\"base_forecasts\"].numpy())\n",
    "        all_reconciled_predictions.append(outputs[\"reconciled_forecasts\"].numpy())\n",
    "        all_anomaly_scores.append(outputs[\"anomaly_scores\"].numpy())\n",
    "        all_hierarchy_levels.append(batch[\"hierarchy_level\"].numpy())\n",
    "\n",
    "# Concatenate results\n",
    "predictions = np.concatenate(all_predictions, axis=0)\n",
    "targets = np.concatenate(all_targets, axis=0)\n",
    "base_predictions = np.concatenate(all_base_predictions, axis=0)\n",
    "reconciled_predictions = np.concatenate(all_reconciled_predictions, axis=0)\n",
    "anomaly_scores = np.concatenate(all_anomaly_scores, axis=0)\n",
    "hierarchy_levels = np.concatenate(all_hierarchy_levels, axis=0)\n",
    "\n",
    "print(f\"Evaluation completed:\")\n",
    "print(f\"  Predictions shape: {predictions.shape}\")\n",
    "print(f\"  Targets shape: {targets.shape}\")\n",
    "print(f\"  Number of samples: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute basic metrics\n",
    "from hierarchical_demand_reconciliation_with_temporal_anomaly_feedback.evaluation.metrics import (\n",
    "    HierarchicalMetrics\n",
    ")\n",
    "\n",
    "# Flatten predictions for metric computation\n",
    "predictions_flat = predictions.reshape(-1)\n",
    "targets_flat = targets.reshape(-1)\n",
    "base_predictions_flat = base_predictions.reshape(-1)\n",
    "reconciled_predictions_flat = reconciled_predictions.reshape(-1)\n",
    "hierarchy_levels_flat = np.repeat(hierarchy_levels, predictions.shape[1])\n",
    "\n",
    "# Initialize metrics\n",
    "hierarchical_metrics = HierarchicalMetrics()\n",
    "\n",
    "# Compute metrics\n",
    "metrics = hierarchical_metrics.compute_all_metrics(\n",
    "    predictions=reconciled_predictions_flat,\n",
    "    targets=targets_flat,\n",
    "    hierarchy_levels=hierarchy_levels_flat,\n",
    "    anomaly_scores=anomaly_scores.flatten(),\n",
    ")\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "key_metrics = ['rmse', 'mae', 'mape', 'smape', 'wrmsse']\n",
    "for metric in key_metrics:\n",
    "    if metric in metrics:\n",
    "        print(f\"{metric.upper()}: {metrics[metric]:.4f}\")\n",
    "\n",
    "print(f\"\\nAnomaly Detection Rate: {metrics.get('anomaly_detection_rate', 'N/A'):.3f}\")\n",
    "print(f\"Samples Evaluated: {metrics.get('n_samples', 'N/A')}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs target scatter plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Overall predictions vs targets\n",
    "axes[0].scatter(targets_flat, reconciled_predictions_flat, alpha=0.5, s=1)\n",
    "min_val = min(targets_flat.min(), reconciled_predictions_flat.min())\n",
    "max_val = max(targets_flat.max(), reconciled_predictions_flat.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "axes[0].set_xlabel('True Values')\n",
    "axes[0].set_ylabel('Predictions')\n",
    "axes[0].set_title('Reconciled Predictions vs Targets')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Base vs reconciled predictions\n",
    "axes[1].scatter(base_predictions_flat, reconciled_predictions_flat, alpha=0.5, s=1, color='green')\n",
    "min_val = min(base_predictions_flat.min(), reconciled_predictions_flat.min())\n",
    "max_val = max(base_predictions_flat.max(), reconciled_predictions_flat.max())\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', label='No change')\n",
    "axes[1].set_xlabel('Base Predictions')\n",
    "axes[1].set_ylabel('Reconciled Predictions')\n",
    "axes[1].set_title('Base vs Reconciled Predictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Anomaly score distribution\n",
    "axes[2].hist(anomaly_scores.flatten(), bins=30, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[2].axvline(0.5, color='red', linestyle='--', label='Threshold')\n",
    "axes[2].set_xlabel('Anomaly Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Anomaly Score Distribution')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by hierarchy level\n",
    "unique_levels = np.unique(hierarchy_levels)\n",
    "level_names = [f\"Level {level}\" for level in unique_levels]\n",
    "level_errors = []\n",
    "level_counts = []\n",
    "\n",
    "for level in unique_levels:\n",
    "    level_mask = hierarchy_levels == level\n",
    "    if np.any(level_mask):\n",
    "        level_preds = reconciled_predictions[level_mask].flatten()\n",
    "        level_targets = targets[level_mask].flatten()\n",
    "        level_error = np.mean(np.abs(level_preds - level_targets))\n",
    "        level_errors.append(level_error)\n",
    "        level_counts.append(np.sum(level_mask))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MAE by hierarchy level\n",
    "bars1 = axes[0].bar(level_names, level_errors, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Hierarchy Level')\n",
    "axes[0].set_ylabel('Mean Absolute Error')\n",
    "axes[0].set_title('Performance by Hierarchy Level')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, error in zip(bars1, level_errors):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{error:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Sample counts by hierarchy level\n",
    "bars2 = axes[1].bar(level_names, level_counts, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Hierarchy Level')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_title('Sample Distribution by Hierarchy Level')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars2, level_counts):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction visualization\n",
    "# Show actual vs predicted time series for a few samples\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select first 4 samples\n",
    "for i in range(min(4, len(predictions))):\n",
    "    sample_target = targets[i, :, 0]  # Shape: (prediction_length,)\n",
    "    sample_base_pred = base_predictions[i, :, 0]\n",
    "    sample_reconciled_pred = reconciled_predictions[i, :, 0]\n",
    "    \n",
    "    time_steps = range(len(sample_target))\n",
    "    \n",
    "    axes[i].plot(time_steps, sample_target, 'b-', linewidth=2, label='Target', marker='o')\n",
    "    axes[i].plot(time_steps, sample_base_pred, 'r--', linewidth=2, label='Base Prediction', marker='s')\n",
    "    axes[i].plot(time_steps, sample_reconciled_pred, 'g-', linewidth=2, label='Reconciled Prediction', marker='^')\n",
    "    \n",
    "    axes[i].set_title(f'Sample {i+1} (Level {hierarchy_levels[i]})')\n",
    "    axes[i].set_xlabel('Time Step')\n",
    "    axes[i].set_ylabel('Sales')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HIERARCHICAL DEMAND RECONCILIATION DEMO SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Data:\")\n",
    "print(f\"  ‚Ä¢ Generated {sales_data['item_id'].nunique()} synthetic items\")\n",
    "print(f\"  ‚Ä¢ {len(sales_data)} total observations\")\n",
    "print(f\"  ‚Ä¢ {sales_data['date'].nunique()} time periods\")\n",
    "print(f\"  ‚Ä¢ Hierarchy: {len(hierarchy_mapping)} levels\")\n",
    "\n",
    "print(\"\\nü§ñ Model:\")\n",
    "print(f\"  ‚Ä¢ Transformer-based hierarchical forecasting\")\n",
    "print(f\"  ‚Ä¢ {trainable_params:,} trainable parameters\")\n",
    "print(f\"  ‚Ä¢ Anomaly detection with attention mechanism\")\n",
    "print(f\"  ‚Ä¢ Reconciliation with coherence constraints\")\n",
    "\n",
    "print(\"\\nüìà Performance:\")\n",
    "for metric in ['rmse', 'mae', 'mape', 'wrmsse']:\n",
    "    if metric in metrics:\n",
    "        print(f\"  ‚Ä¢ {metric.upper()}: {metrics[metric]:.4f}\")\n",
    "\n",
    "print(f\"  ‚Ä¢ Anomaly detection rate: {metrics.get('anomaly_detection_rate', 0):.1%}\")\n",
    "print(f\"  ‚Ä¢ Evaluated on {metrics.get('n_samples', 0)} samples\")\n",
    "\n",
    "print(\"\\nüéØ Key Features Demonstrated:\")\n",
    "print(\"  ‚úì Hierarchical time series forecasting\")\n",
    "print(\"  ‚úì Temporal anomaly detection\")\n",
    "print(\"  ‚úì Forecasting reconciliation\")\n",
    "print(\"  ‚úì Multi-level hierarchy handling\")\n",
    "print(\"  ‚úì Attention-based temporal modeling\")\n",
    "\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"  ‚Ä¢ Run full training: `python scripts/train.py --synthetic-data`\")\n",
    "print(\"  ‚Ä¢ Evaluate on M5 data: `python scripts/train.py --data-path data/m5`\")\n",
    "print(\"  ‚Ä¢ Tune hyperparameters in configs/default.yaml\")\n",
    "print(\"  ‚Ä¢ Analyze reconciliation improvements\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Demo completed successfully! üéâ\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}